{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3dotnhNCKsl"
      },
      "source": [
        "## Lab 7 (Image Processing using Convolutional Neural Networks)\n",
        "- CIFAR10 dataset (see https://www.cs.toronto.edu/~kriz/cifar.html for more info)\n",
        "- 60K images: 50K train, 10K test\n",
        "- 10 classes: 'plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\n",
        "- Perform multi-class classification with evaluation accuracy on EACH class\n",
        "\n",
        "**CONNECT TO GPU** before continuing, but just CPU is also fine, it might be a bit slow.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ8bPH1OCM5h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6502b4fb-ab18-44c3-9558-ddebd3ed159b"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "# Hyper parameters\n",
        "num_epochs = 4\n",
        "batch_size = 4\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Download and prepare dataset\n",
        "# Transform them to tensors and normalise them\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "     ])\n",
        "\n",
        "# 2.2 Download data\n",
        "train_set = torchvision.datasets.CIFAR10(\"./\", train=True, download=True, transform=transform)\n",
        "test_set = torchvision.datasets.CIFAR10(\"./\", train=False, download=True, transform=transform)\n",
        "\n",
        "# 2.3 Use DataLoader to get batches and shuffle\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Q1. Why are there 3 values in each list of the Normalize() function? What does each value and each list represent?\n",
        "\n",
        "# The Normalize() function normalize a tensor image with a mean and standard\n",
        "# deviation for n channels, meaning this normalize each channel of the input.\n",
        "# So the value in the first list is the mean and the values in the\n",
        "# second list is the standard deviation. Each list have three values due to the\n",
        "# dimensions of the image.\n",
        "\n",
        "\n",
        "# There are three since there are three channels.\n",
        "# The first tuple gives the mean value of color channel in the image data.\n",
        "# The second tuple gives the standard deviation of each color channel in the image.\n",
        "# Each image pixel value in a particular channel will subtrac tthe provided mean\n",
        "# and the std to get a normalized value."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuhDiijV7CNT"
      },
      "source": [
        "### Inspect the Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "032BETSy6a2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ed6cdf0-f978-4325-870d-343916c5df3f"
      },
      "source": [
        "# Access the first data sample in the train_set using next(iter())\n",
        "batch = next(iter(train_loader))\n",
        "print(f'Image values: \\n{batch}')\n",
        "print(f'Length: {len(batch)}')\n",
        "print(f'Type: {type(batch)}')\n",
        "\n",
        "# This means the data contains image-label pairs\n",
        "# Unpack them\n",
        "images, labels = batch\n",
        "# Same as these two lines:\n",
        "# image = batch[0]\n",
        "# label = batch[1]\n",
        "\n",
        "\n",
        "print(images.shape)\n",
        "print(labels)\n",
        "\n",
        "# Q2. What is the range of the values for the normalised image pixels?\n",
        "\n",
        "# Then range is from -1 to 1.\n",
        "\n",
        "# Q3. What does each index value of the shape of the image represent?\n",
        "\n",
        "# Batch size, Channel, and Dimensions (height and width) of image\n",
        "\n",
        "# Q4. What do the label values represent?\n",
        "\n",
        "# The label values represent the classes of the image.\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image values: \n",
            "[tensor([[[[-0.1922, -0.1059, -0.8039,  ..., -0.2706, -0.2392, -0.4039],\n",
            "          [-0.2941, -0.1216, -0.2941,  ..., -0.2000, -0.2549, -0.3098],\n",
            "          [-0.0039, -0.1373, -0.0118,  ..., -0.2863, -0.4118, -0.2784],\n",
            "          ...,\n",
            "          [-0.0745,  0.2157,  0.3882,  ...,  0.1059,  0.1216,  0.0824],\n",
            "          [-0.1294,  0.3176,  0.5137,  ...,  0.1137,  0.2549,  0.0745],\n",
            "          [-0.1608,  0.3176,  0.5059,  ...,  0.0667,  0.4039,  0.1686]],\n",
            "\n",
            "         [[-0.1373, -0.0588, -0.7569,  ..., -0.1843, -0.1686, -0.3490],\n",
            "          [-0.2314, -0.0588, -0.2235,  ..., -0.1294, -0.1843, -0.2549],\n",
            "          [ 0.0588, -0.0667,  0.0588,  ..., -0.2157, -0.3412, -0.2157],\n",
            "          ...,\n",
            "          [ 0.0039,  0.3098,  0.4588,  ...,  0.1294,  0.1373,  0.1137],\n",
            "          [-0.0667,  0.3961,  0.5686,  ...,  0.1529,  0.2941,  0.1059],\n",
            "          [-0.1216,  0.3725,  0.5529,  ...,  0.1373,  0.4667,  0.2157]],\n",
            "\n",
            "         [[-0.1608, -0.0745, -0.7569,  ..., -0.2157, -0.1922, -0.3725],\n",
            "          [-0.2627, -0.0667, -0.2314,  ..., -0.1529, -0.2078, -0.2784],\n",
            "          [ 0.0196, -0.0902,  0.0510,  ..., -0.2471, -0.3647, -0.2314],\n",
            "          ...,\n",
            "          [-0.0118,  0.2941,  0.4039,  ...,  0.0745,  0.0667,  0.0510],\n",
            "          [-0.0980,  0.3647,  0.5216,  ...,  0.0980,  0.2157,  0.0353],\n",
            "          [-0.1765,  0.3255,  0.5137,  ...,  0.1059,  0.4039,  0.1529]]],\n",
            "\n",
            "\n",
            "        [[[ 0.5922,  0.5922,  0.6078,  ...,  0.1686,  0.2627,  0.5922],\n",
            "          [ 0.6314,  0.6314,  0.6392,  ...,  0.2392,  0.2235,  0.5529],\n",
            "          [ 0.6392,  0.6392,  0.6392,  ...,  0.2863,  0.0353,  0.2471],\n",
            "          ...,\n",
            "          [ 0.1843, -0.1373, -0.4353,  ..., -0.2000, -0.3176, -0.3961],\n",
            "          [ 0.3176,  0.3176, -0.2000,  ..., -0.4588, -0.4431, -0.4353],\n",
            "          [-0.1765,  0.0824, -0.1216,  ..., -0.4039, -0.3725, -0.3412]],\n",
            "\n",
            "         [[ 0.7569,  0.7569,  0.7725,  ...,  0.1922,  0.2941,  0.6235],\n",
            "          [ 0.7882,  0.7882,  0.7961,  ...,  0.2627,  0.2549,  0.5843],\n",
            "          [ 0.7804,  0.7804,  0.7804,  ...,  0.3020,  0.0510,  0.2627],\n",
            "          ...,\n",
            "          [ 0.1608, -0.1529, -0.4510,  ..., -0.2078, -0.3255, -0.4039],\n",
            "          [ 0.2941,  0.3020, -0.2157,  ..., -0.4667, -0.4510, -0.4353],\n",
            "          [-0.2000,  0.0667, -0.1373,  ..., -0.4039, -0.3804, -0.3412]],\n",
            "\n",
            "         [[ 0.9843,  0.9686,  0.9686,  ...,  0.2784,  0.3804,  0.7176],\n",
            "          [ 1.0000,  0.9922,  0.9843,  ...,  0.3412,  0.3255,  0.6627],\n",
            "          [ 0.9922,  0.9765,  0.9608,  ...,  0.3647,  0.1216,  0.3255],\n",
            "          ...,\n",
            "          [ 0.2157, -0.1137, -0.4353,  ..., -0.1608, -0.2784, -0.3569],\n",
            "          [ 0.3490,  0.3412, -0.1922,  ..., -0.4275, -0.4039, -0.4039],\n",
            "          [-0.1451,  0.1059, -0.1216,  ..., -0.3804, -0.3490, -0.3255]]],\n",
            "\n",
            "\n",
            "        [[[ 0.5686,  0.9529,  0.9608,  ...,  0.9451,  0.9451,  0.9216],\n",
            "          [-0.0745,  0.8510,  0.9529,  ...,  0.9373,  0.9451,  0.9608],\n",
            "          [-0.0039,  0.8902,  0.9608,  ...,  0.9686,  0.9765,  0.9843],\n",
            "          ...,\n",
            "          [-0.1216, -0.1529, -0.1608,  ..., -0.0745, -0.0667, -0.1059],\n",
            "          [-0.2941, -0.3020, -0.2941,  ..., -0.0667, -0.0510, -0.0824],\n",
            "          [-0.2784, -0.2941, -0.2627,  ..., -0.0039, -0.0431, -0.0431]],\n",
            "\n",
            "         [[ 0.5294,  0.9373,  0.9529,  ...,  0.9451,  0.9451,  0.9216],\n",
            "          [-0.1137,  0.8275,  0.9373,  ...,  0.9373,  0.9451,  0.9608],\n",
            "          [-0.0431,  0.8588,  0.9373,  ...,  0.9686,  0.9765,  0.9843],\n",
            "          ...,\n",
            "          [-0.1922, -0.2235, -0.2314,  ..., -0.1765, -0.1686, -0.1922],\n",
            "          [-0.3569, -0.3647, -0.3569,  ..., -0.1686, -0.1451, -0.1686],\n",
            "          [-0.3412, -0.3569, -0.3255,  ..., -0.1059, -0.1451, -0.1373]],\n",
            "\n",
            "         [[ 0.5843,  0.9608,  0.9608,  ...,  0.9451,  0.9451,  0.9216],\n",
            "          [-0.0510,  0.8588,  0.9451,  ...,  0.9373,  0.9451,  0.9608],\n",
            "          [ 0.0118,  0.8824,  0.9451,  ...,  0.9686,  0.9765,  0.9843],\n",
            "          ...,\n",
            "          [-0.1843, -0.2157, -0.2235,  ..., -0.2235, -0.2000, -0.2000],\n",
            "          [-0.3647, -0.3804, -0.3647,  ..., -0.2314, -0.1922, -0.1765],\n",
            "          [-0.3569, -0.3725, -0.3412,  ..., -0.1765, -0.1843, -0.1373]]],\n",
            "\n",
            "\n",
            "        [[[-0.1059, -0.3490, -0.5451,  ..., -0.9608, -0.9373, -0.9059],\n",
            "          [-0.5765, -0.6314, -0.6471,  ..., -0.9608, -0.9608, -0.9686],\n",
            "          [-0.6235, -0.0980,  0.0902,  ..., -0.9608, -0.9608, -0.9608],\n",
            "          ...,\n",
            "          [-0.9059, -0.9137, -0.9137,  ..., -0.9608, -0.9451, -0.9294],\n",
            "          [-0.8980, -0.9059, -0.9216,  ..., -0.9216, -0.9216, -0.9216],\n",
            "          [-0.9373, -0.9373, -0.9529,  ..., -0.9294, -0.9137, -0.8980]],\n",
            "\n",
            "         [[-0.1294, -0.4039, -0.6235,  ..., -0.9608, -0.9373, -0.9059],\n",
            "          [-0.6549, -0.7255, -0.7490,  ..., -0.9608, -0.9608, -0.9686],\n",
            "          [-0.7176, -0.2157, -0.0353,  ..., -0.9608, -0.9608, -0.9608],\n",
            "          ...,\n",
            "          [-0.9059, -0.9137, -0.9137,  ..., -0.9608, -0.9451, -0.9294],\n",
            "          [-0.8980, -0.9059, -0.9137,  ..., -0.9216, -0.9216, -0.9216],\n",
            "          [-0.9373, -0.9373, -0.9451,  ..., -0.9294, -0.9137, -0.8980]],\n",
            "\n",
            "         [[-0.1059, -0.3961, -0.6314,  ..., -0.9451, -0.9216, -0.8902],\n",
            "          [-0.6627, -0.7569, -0.7882,  ..., -0.9451, -0.9451, -0.9529],\n",
            "          [-0.7490, -0.2627, -0.0902,  ..., -0.9451, -0.9451, -0.9451],\n",
            "          ...,\n",
            "          [-0.9059, -0.9137, -0.9137,  ..., -0.9608, -0.9451, -0.9294],\n",
            "          [-0.8902, -0.8980, -0.9059,  ..., -0.9216, -0.9216, -0.9216],\n",
            "          [-0.9216, -0.9216, -0.9294,  ..., -0.9294, -0.9137, -0.8980]]]]), tensor([6, 9, 8, 3])]\n",
            "Length: 2\n",
            "Type: <class 'list'>\n",
            "torch.Size([4, 3, 32, 32])\n",
            "tensor([6, 9, 8, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71_0Ci7gZkqV"
      },
      "source": [
        "### View some images\n",
        "- Note that images have been normalised and may not look very clear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zwz0kGvfuL6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "57426229-8115-4335-bd3c-7f60e9cfc200"
      },
      "source": [
        "# Create a grid\n",
        "plt.figure(figsize=(12,12))\n",
        "grid = torchvision.utils.make_grid(tensor=images, nrow=4) # nrow = number of images displayed in each row\n",
        "\n",
        "print(f\"class labels: {labels}\")\n",
        "\n",
        "# Use grid.permute() to transpose the grid so that the axes meet the specifications required by\n",
        "# plt.imshow(), which are [height, width, channels]. PyTorch dimensions are [channels, height, width].\n",
        "plt.imshow(grid.permute(1,2,0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class labels: tensor([6, 9, 8, 3])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f2659a3ef50>"
            ]
          },
          "metadata": {},
          "execution_count": 147
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x864 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVMti_g2J-W5"
      },
      "source": [
        "## CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWePXqBQKAp7"
      },
      "source": [
        "class Test(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5, padding = 1) # input 3 channel, apply 6 filters, with each filter being a 5x5\n",
        "    self.relu = nn.ReLU()\n",
        "    self.pool = nn.MaxPool2d(2, 2) # kernel size = 2, stride = 2\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5) # input 6 channels from conv1,\n",
        "    # flatten 3D tensor to 1D tensor\n",
        "    self.fc1 = nn.Linear(16*5*5, 128) # Q8. Fill out the correct input dimensions\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc3 = nn.Linear(64, 10) # final output matches num_classes\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Conv + ReLU + pool\n",
        "    print(f'Input shape: {x.shape}')\n",
        "    out = self.conv1(x)\n",
        "    print(f'After Conv1: {out.shape}')\n",
        "    print(f'Padding: {self.conv1.padding}')\n",
        "    out = self.pool(F.relu(out))\n",
        "    print(f'After Pool1: {out.shape}')\n",
        "    out = self.conv2(out)\n",
        "    print(f'After Conv2: {out.shape}')\n",
        "    out = self.pool(F.relu(out))\n",
        "    print(f'After Pool2: {out.shape}')\n",
        "    # Flatten it before fc1\n",
        "    out = out.reshape(-1, 16*5*5) # Q8. Fill out the correct dimension after -1\n",
        "    print(f'Before fc1: {out.shape}')\n",
        "    out = self.fc1(out)\n",
        "    out = self.relu(out)\n",
        "    print(f'After fc1: {out.shape}')\n",
        "    out = self.fc2(out)\n",
        "    out = self.relu(out)\n",
        "    print(f'After fc2: {out.shape}')\n",
        "    out = self.fc3(out) # NO softmax as it will be included in CrossEntropyLoss\n",
        "    print(f'After fc3: {out.shape}')\n",
        "    return out\n",
        "\n",
        "\n",
        "model = Test().to(device)\n",
        "# Let's view the softmax output\n",
        "probs = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "# Q5. What do the three arguments of the first convolutional layer, conv1 represent (3,6,5)?\n",
        "\n",
        "# conv1 applies a 2D convulution over an input. So the numbers represents the\n",
        "# in_channels (number of channels in the input image), out_channels (number of\n",
        "# channels produced by the convolution), and the kernel_size (size of the\n",
        "# convolving kernel stride).\n",
        "\n",
        "# input = 3 channels, output = 6 meaning there is 6 filter map, filter size = 5x5\n",
        "\n",
        "# Q6. Explain the arguments of the second convolutional layer, conv2 (6, 16, 5)\n",
        "\n",
        "# The 6 represents the channels of the input from the first convolutional layer.\n",
        "# The 16 represents the number of channels produced by the convolution. The 5\n",
        "# presents the size of the convolving kernel stride.\n",
        "\n",
        "# Q7. Figure out the convolved image size after conv1\n",
        "# Convolved image size = ((input_width - filter_size + 2 * padding) / stride) + 1\n",
        "\n",
        "# convolved image size = ((32- 5 + 2*0)/1) +1 = 28\n",
        "\n",
        "# Q8. Figure out the input size to the first fcn layer and fill out the code above in init() and forward()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZk7fcriftFm"
      },
      "source": [
        "### Run through a sample batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW7t-qz-FjGp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1261a8bb-fa4f-4f78-85cc-a12471cee859"
      },
      "source": [
        "sample = next(iter(train_loader))\n",
        "\n",
        "images, labels = sample\n",
        "\n",
        "images = images.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "output = model(images)\n",
        "print(f'Output shape: {output.shape}')\n",
        "print(f'Softmax outputs:\\n {probs(output)}')\n",
        "\n",
        "\n",
        "# Q9. Explain the shape of the output after conv1\n",
        "\n",
        "# After conv1, the shape was [4, 6, 28, 28]. This tells us about the number of\n",
        "# channels and dimensions of the image.\n",
        "\n",
        "# Q10. What does the pooling do to the dimensions of the feature images here?\n",
        "\n",
        "# Pooling compresses the image. Width and height halved.\n",
        "\n",
        "# Q11. Add padding=1 to conv1 and rerun the last two code cells. How did padding affect the dimensions of the feature images?\n",
        "\n",
        "# The dimensions of the images are larger. After conv1 the dimensions are 30x30\n",
        "# and not 28x28. The dimensions of the images up until after conv2 is larger.\n",
        "\n",
        "\n",
        "# Q12. What is represented by each list returned by Softmax outputs?\n",
        "\n",
        "# The probability of the classes of images in the order of\n",
        "# 'plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'.\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([4, 3, 32, 32])\n",
            "After Conv1: torch.Size([4, 6, 30, 30])\n",
            "Padding: (1, 1)\n",
            "After Pool1: torch.Size([4, 6, 15, 15])\n",
            "After Conv2: torch.Size([4, 16, 11, 11])\n",
            "After Pool2: torch.Size([4, 16, 5, 5])\n",
            "Before fc1: torch.Size([4, 400])\n",
            "After fc1: torch.Size([4, 128])\n",
            "After fc2: torch.Size([4, 64])\n",
            "After fc3: torch.Size([4, 10])\n",
            "Output shape: torch.Size([4, 10])\n",
            "Softmax outputs:\n",
            " tensor([[0.0959, 0.1097, 0.1027, 0.0991, 0.1122, 0.0993, 0.0925, 0.0947, 0.0954,\n",
            "         0.0984],\n",
            "        [0.0962, 0.1096, 0.1025, 0.0991, 0.1124, 0.0987, 0.0934, 0.0951, 0.0947,\n",
            "         0.0982],\n",
            "        [0.0961, 0.1086, 0.1035, 0.0995, 0.1134, 0.0997, 0.0929, 0.0934, 0.0948,\n",
            "         0.0980],\n",
            "        [0.0969, 0.1107, 0.1028, 0.0984, 0.1124, 0.0987, 0.0927, 0.0950, 0.0945,\n",
            "         0.0980]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQj3gsf7Y6Ql"
      },
      "source": [
        "\n",
        "### Let's Train!\n",
        "- Now that we know and understand how CNNs work, let's put everything together for CIFAR-10 dataset\n",
        "  - Download the data in batches and normalisation with shuffling\n",
        "  - Build a model with 2 CNN layers containing ReLU and pooling\n",
        "  - Passing the feature images to 3 fully connected layers (FCNs) also containing RELU activation\n",
        "  - The final layer has 10 units to reprsent the number of output classes\n",
        "  - Use Binary Cross Entropy Loss and SGD optimiser\n",
        "  - Evaluate the model on the test data on EACH class\n",
        "\n",
        "**IMPORTANT!** Fill out the missing code below before training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1_4tKL4X3WQ"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "    # flatten 3D tensor to 1D tensor\n",
        "    self.fc1 = nn.Linear(16*5*5, 128) # TODO\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc3 = nn.Linear(64, 10) # final output matches num_classes\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Conv + ReLU + pool\n",
        "    out = self.pool(F.relu(self.conv1(x)))\n",
        "    out = self.pool(F.relu(self.conv2(out)))\n",
        "    # Flatten it before fc1\n",
        "    out = out.reshape(-1, 16*5*5) # TODO\n",
        "    out = F.relu(self.fc1(out))\n",
        "    out = F.relu(self.fc2(out))\n",
        "    out = self.fc3(out) # NO softmax as it will be included in CrossEntropyLoss\n",
        "    return out\n",
        "\n",
        "\n",
        "model = CNN().to(device)\n",
        "\n",
        "# Q13. Use the Cross Entropy Loss for this task (UNCOMMENT & COMPLETE CODE BELOW)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Q14. Use the Stochastic Gradient Descent (SGD) optimiser, this time ADD momentum=0.9 (UNCOMMENT & COMPLETE CODE BELOW)\n",
        "opt = torch.optim.SGD(model.parameters(), momentum = 0.9, lr = learning_rate)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zlg2FFaJKppP"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e15E85ZQKr1R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8c3f0d9-a405-4af7-9cdd-99176f300f07"
      },
      "source": [
        "n_total_steps = len(train_set)\n",
        "n_iterations = -(-n_total_steps // batch_size) # ceiling division\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    #print(images.shape) # [4,3,32,32] batch size, channels, img dim\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Backward pass and Optimise\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    # Print\n",
        "    if (i+1) % 1000 == 0:\n",
        "      print(f'Epoch {epoch+1}/{num_epochs}, Iteration {i+1}/{n_iterations}, Loss={loss.item():.4f} ')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Iteration 1000/6250, Loss=2.0976 \n",
            "Epoch 1/5, Iteration 2000/6250, Loss=1.6308 \n",
            "Epoch 1/5, Iteration 3000/6250, Loss=2.0121 \n",
            "Epoch 1/5, Iteration 4000/6250, Loss=1.8918 \n",
            "Epoch 1/5, Iteration 5000/6250, Loss=1.4266 \n",
            "Epoch 1/5, Iteration 6000/6250, Loss=1.5908 \n",
            "Epoch 2/5, Iteration 1000/6250, Loss=1.4374 \n",
            "Epoch 2/5, Iteration 2000/6250, Loss=1.7278 \n",
            "Epoch 2/5, Iteration 3000/6250, Loss=1.3714 \n",
            "Epoch 2/5, Iteration 4000/6250, Loss=0.8623 \n",
            "Epoch 2/5, Iteration 5000/6250, Loss=1.4013 \n",
            "Epoch 2/5, Iteration 6000/6250, Loss=1.7017 \n",
            "Epoch 3/5, Iteration 1000/6250, Loss=1.8152 \n",
            "Epoch 3/5, Iteration 2000/6250, Loss=0.9941 \n",
            "Epoch 3/5, Iteration 3000/6250, Loss=1.5724 \n",
            "Epoch 3/5, Iteration 4000/6250, Loss=1.4245 \n",
            "Epoch 3/5, Iteration 5000/6250, Loss=1.2327 \n",
            "Epoch 3/5, Iteration 6000/6250, Loss=1.3907 \n",
            "Epoch 4/5, Iteration 1000/6250, Loss=0.9945 \n",
            "Epoch 4/5, Iteration 2000/6250, Loss=0.9815 \n",
            "Epoch 4/5, Iteration 3000/6250, Loss=2.2211 \n",
            "Epoch 4/5, Iteration 4000/6250, Loss=1.2625 \n",
            "Epoch 4/5, Iteration 5000/6250, Loss=0.8875 \n",
            "Epoch 4/5, Iteration 6000/6250, Loss=0.9526 \n",
            "Epoch 5/5, Iteration 1000/6250, Loss=1.1738 \n",
            "Epoch 5/5, Iteration 2000/6250, Loss=1.5541 \n",
            "Epoch 5/5, Iteration 3000/6250, Loss=1.2645 \n",
            "Epoch 5/5, Iteration 4000/6250, Loss=0.5512 \n",
            "Epoch 5/5, Iteration 5000/6250, Loss=1.1425 \n",
            "Epoch 5/5, Iteration 6000/6250, Loss=1.0074 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HahXHRsSMo6H"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIuqYw8JMqy-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adf3ec58-81c2-436b-cabe-8261c663b23d"
      },
      "source": [
        "# Deactivate the autograd engine to reduce memory usage and speed up computations (backprop disabled).\n",
        "with torch.no_grad():\n",
        "  n_correct = 0\n",
        "  n_samples = 0\n",
        "  n_class_correct = [0 for i in range(10)]\n",
        "  n_class_samples = [0 for i in range(10)]\n",
        "\n",
        "\n",
        "  # Loop through test set\n",
        "  for images, labels in test_loader:\n",
        "    # Put images on GPU\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    # Run on trained model\n",
        "    outputs = model(images)\n",
        "\n",
        "    # Get predictions\n",
        "    # torch.max() returns actual probability value (ignored) and index or class label (selected)\n",
        "    _, y_preds = torch.max(outputs, 1)\n",
        "    n_samples += labels.size(0) # different to FFNN\n",
        "    n_correct += (y_preds == labels).sum().item()\n",
        "\n",
        "    # Keep track of each class\n",
        "    for i in range(batch_size):\n",
        "      label = labels[i]\n",
        "      pred = y_preds[i]\n",
        "      if (label == pred):\n",
        "        n_class_correct[label] += 1\n",
        "      n_class_samples[label] += 1\n",
        "\n",
        "  # Print accuracy\n",
        "  acc = 100.0 * n_correct / n_samples\n",
        "  print(f'Test Accuracy of the WHOLE CNN = {acc} %')\n",
        "\n",
        "  for i in range(len(classes)):\n",
        "    acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "    print(f'Accuracy of {classes[i]}: {acc} %')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy of the WHOLE CNN = 58.26 %\n",
            "Accuracy of plane: 71.1 %\n",
            "Accuracy of car: 67.1 %\n",
            "Accuracy of bird: 44.4 %\n",
            "Accuracy of cat: 34.2 %\n",
            "Accuracy of deer: 35.2 %\n",
            "Accuracy of dog: 65.1 %\n",
            "Accuracy of frog: 84.4 %\n",
            "Accuracy of horse: 54.7 %\n",
            "Accuracy of ship: 48.9 %\n",
            "Accuracy of truck: 77.5 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TKj_ZV-Dzzn"
      },
      "source": [
        "# Q15. Why don't we need to reshape the input images when training and testing?\n",
        "\n",
        "# Because these imput images have already been normalized.\n",
        "\n",
        "# images are not fed into FC layers at first, convolutions will work on the wholeimage region and they will\n",
        "\n",
        "# Q16. Try to improve the model performance, e.g. by increasing the epochs, changing batch size, adding convolutions, etc.\n",
        "# Provide the code chunk showing the improved accuracy on the test set below. What changes did you make?\n",
        "\n",
        "# During the first run, the total model accuracy was 24.1%. I increased the\n",
        "# number of epoch and batch size. Changing the number increase the total model\n",
        "# accuracy to 58.26%.\n",
        "\n",
        "# Hyper parameters\n",
        "num_epochs = 5\n",
        "batch_size = 8\n",
        "learning_rate = 0.001"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}